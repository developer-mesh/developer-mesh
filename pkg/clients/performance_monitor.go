package clients

import (
	"fmt"
	"math"
	"sort"
	"sync"
	"time"

	"github.com/developer-mesh/developer-mesh/pkg/observability"
)

// PerformanceMonitor tracks and analyzes performance metrics
type PerformanceMonitor struct {
	mu     sync.RWMutex
	logger observability.Logger

	// Configuration
	config MonitorConfig

	// Metrics collection
	collector *MetricsCollector

	// Performance analysis
	analyzer *PerformanceAnalyzer

	// Alerting
	alertManager *AlertManager

	// Historical data
	history *MetricsHistory

	// Shutdown management
	shutdown chan struct{}
	wg       sync.WaitGroup
}

// MonitorConfig defines monitoring configuration
type MonitorConfig struct {
	CollectionInterval    time.Duration `json:"collection_interval"`
	HistoryRetention      time.Duration `json:"history_retention"`
	EnableAlerting        bool          `json:"enable_alerting"`
	EnableAutoTuning      bool          `json:"enable_auto_tuning"`
	MetricsBufferSize     int           `json:"metrics_buffer_size"`
	PercentileCalculation bool          `json:"percentile_calculation"`
}

// MetricsCollector collects performance metrics
type MetricsCollector struct {
	mu sync.RWMutex

	// Request metrics
	requestMetrics *RequestMetrics

	// Cache metrics
	cacheMetrics *CachePerformanceMetrics

	// System metrics
	systemMetrics *SystemMetrics

	// Custom metrics
	customMetrics map[string]*CustomMetric
}

// RequestMetrics tracks request performance
type RequestMetrics struct {
	mu sync.RWMutex

	// Timing metrics
	responseTimes []time.Duration
	maxBufferSize int

	// Throughput metrics
	requestCount     int64
	bytesTransferred int64
	startTime        time.Time

	// Error metrics
	errorCount   int64
	errorsByType map[string]int64

	// Latency breakdown
	dnsLatency      []time.Duration
	connectLatency  []time.Duration
	tlsLatency      []time.Duration
	serverLatency   []time.Duration
	transferLatency []time.Duration
}

// CachePerformanceMetrics tracks cache performance
type CachePerformanceMetrics struct {
	mu sync.RWMutex

	// Hit rates
	l1HitRate      float64
	l2HitRate      float64
	overallHitRate float64

	// Latencies
	l1Latencies []time.Duration
	l2Latencies []time.Duration

	// Efficiency
	memoryUsage      int64
	evictionRate     float64
	warmupEfficiency float64
}

// SystemMetrics tracks system resource usage
type SystemMetrics struct {
	mu sync.RWMutex

	// Resource usage
	cpuUsage        float64
	memoryUsage     int64
	goroutineCount  int
	connectionCount int

	// Network metrics
	networkBandwidth int64
	packetLoss       float64

	// Timestamps
	lastUpdated time.Time
}

// CustomMetric allows tracking custom metrics
type CustomMetric struct {
	Name        string
	Type        string // counter, gauge, histogram
	Value       float64
	Values      []float64
	LastUpdated time.Time
}

// PerformanceAnalyzer analyzes performance data
type PerformanceAnalyzer struct {
	mu sync.RWMutex

	// Analysis results
	currentAnalysis  *AnalysisResult
	historicalTrends map[string]*TrendData

	// Configuration
	windowSize       time.Duration
	analysisInterval time.Duration
}

// AnalysisResult contains performance analysis results
type AnalysisResult struct {
	Timestamp time.Time

	// Response time analysis
	AvgResponseTime time.Duration
	P50ResponseTime time.Duration
	P95ResponseTime time.Duration
	P99ResponseTime time.Duration
	MaxResponseTime time.Duration

	// Throughput analysis
	RequestsPerSecond float64
	BytesPerSecond    float64

	// Error analysis
	ErrorRate float64
	TopErrors []ErrorSummary

	// Cache analysis
	CacheEfficiency float64
	CacheSavings    time.Duration

	// Recommendations
	Recommendations []Recommendation

	// Score
	PerformanceScore float64
}

// TrendData tracks performance trends over time
type TrendData struct {
	MetricName string
	DataPoints []DataPoint
	Trend      string // improving, degrading, stable
	ChangeRate float64
}

// DataPoint represents a single metric measurement
type DataPoint struct {
	Timestamp time.Time
	Value     float64
}

// ErrorSummary summarizes error information
type ErrorSummary struct {
	ErrorType string
	Count     int64
	Rate      float64
}

// Recommendation provides performance improvement suggestions
type Recommendation struct {
	Type        string
	Priority    string // high, medium, low
	Description string
	Impact      string
	Action      string
}

// AlertManager manages performance alerts
type AlertManager struct {
	mu sync.RWMutex

	// Alert configuration
	thresholds   map[string]AlertThreshold
	activeAlerts map[string]*Alert
	alertHistory []Alert

	// Alert callbacks
	alertCallbacks []AlertCallback
}

// AlertThreshold defines alert triggering conditions
type AlertThreshold struct {
	MetricName     string
	ThresholdValue float64
	Comparison     string // greater, less, equal
	Duration       time.Duration
	Severity       string // critical, warning, info
}

// Alert represents an active alert
type Alert struct {
	ID           string
	MetricName   string
	CurrentValue float64
	Threshold    float64
	Severity     string
	TriggeredAt  time.Time
	ResolvedAt   *time.Time
	Message      string
}

// AlertCallback is called when an alert is triggered
type AlertCallback func(alert Alert)

// MetricsHistory stores historical metrics
type MetricsHistory struct {
	mu sync.RWMutex

	// Time-series data
	timeSeries      map[string]*TimeSeries
	retentionPeriod time.Duration

	// Aggregated data
	hourlyAggregates map[string][]AggregateData
	dailyAggregates  map[string][]AggregateData
}

// TimeSeries stores time-series metric data
type TimeSeries struct {
	Name       string
	DataPoints []DataPoint
	MaxPoints  int
}

// AggregateData stores aggregated metric data
type AggregateData struct {
	Timestamp time.Time
	Min       float64
	Max       float64
	Avg       float64
	Sum       float64
	Count     int64
}

// DefaultMonitorConfig returns default monitoring configuration
func DefaultMonitorConfig() MonitorConfig {
	return MonitorConfig{
		CollectionInterval:    10 * time.Second,
		HistoryRetention:      24 * time.Hour,
		EnableAlerting:        true,
		EnableAutoTuning:      true,
		MetricsBufferSize:     10000,
		PercentileCalculation: true,
	}
}

// NewPerformanceMonitor creates a new performance monitor
func NewPerformanceMonitor(config MonitorConfig, logger observability.Logger) *PerformanceMonitor {
	monitor := &PerformanceMonitor{
		logger:   logger,
		config:   config,
		shutdown: make(chan struct{}),
	}

	// Initialize metrics collector
	monitor.collector = &MetricsCollector{
		requestMetrics: &RequestMetrics{
			responseTimes:   make([]time.Duration, 0, config.MetricsBufferSize),
			maxBufferSize:   config.MetricsBufferSize,
			errorsByType:    make(map[string]int64),
			dnsLatency:      make([]time.Duration, 0, 100),
			connectLatency:  make([]time.Duration, 0, 100),
			tlsLatency:      make([]time.Duration, 0, 100),
			serverLatency:   make([]time.Duration, 0, 100),
			transferLatency: make([]time.Duration, 0, 100),
			startTime:       time.Now(),
		},
		cacheMetrics: &CachePerformanceMetrics{
			l1Latencies: make([]time.Duration, 0, 100),
			l2Latencies: make([]time.Duration, 0, 100),
		},
		systemMetrics: &SystemMetrics{
			lastUpdated: time.Now(),
		},
		customMetrics: make(map[string]*CustomMetric),
	}

	// Initialize performance analyzer
	monitor.analyzer = &PerformanceAnalyzer{
		historicalTrends: make(map[string]*TrendData),
		windowSize:       5 * time.Minute,
		analysisInterval: 1 * time.Minute,
	}

	// Initialize alert manager
	if config.EnableAlerting {
		monitor.alertManager = &AlertManager{
			thresholds:     make(map[string]AlertThreshold),
			activeAlerts:   make(map[string]*Alert),
			alertHistory:   make([]Alert, 0, 100),
			alertCallbacks: make([]AlertCallback, 0),
		}

		// Set default alert thresholds
		monitor.setDefaultAlertThresholds()
	}

	// Initialize metrics history
	monitor.history = &MetricsHistory{
		timeSeries:       make(map[string]*TimeSeries),
		retentionPeriod:  config.HistoryRetention,
		hourlyAggregates: make(map[string][]AggregateData),
		dailyAggregates:  make(map[string][]AggregateData),
	}

	// Start monitoring workers
	monitor.wg.Add(2)
	go monitor.collectionWorker()
	go monitor.analysisWorker()

	return monitor
}

// RecordRequest records a request's performance metrics
func (m *PerformanceMonitor) RecordRequest(duration time.Duration, bytes int64, err error) {
	m.collector.requestMetrics.mu.Lock()
	defer m.collector.requestMetrics.mu.Unlock()

	// Record response time
	m.collector.requestMetrics.responseTimes = append(m.collector.requestMetrics.responseTimes, duration)
	if len(m.collector.requestMetrics.responseTimes) > m.collector.requestMetrics.maxBufferSize {
		// Remove oldest entries
		m.collector.requestMetrics.responseTimes = m.collector.requestMetrics.responseTimes[100:]
	}

	// Update counters
	m.collector.requestMetrics.requestCount++
	m.collector.requestMetrics.bytesTransferred += bytes

	// Record error if present
	if err != nil {
		m.collector.requestMetrics.errorCount++
		errorType := fmt.Sprintf("%T", err)
		m.collector.requestMetrics.errorsByType[errorType]++
	}
}

// RecordCacheMetrics records cache performance metrics
func (m *PerformanceMonitor) RecordCacheMetrics(l1Hit, l2Hit bool, l1Latency, l2Latency time.Duration) {
	m.collector.cacheMetrics.mu.Lock()
	defer m.collector.cacheMetrics.mu.Unlock()

	// Update hit rates (using exponential moving average)
	alpha := 0.1 // Smoothing factor

	if l1Hit {
		m.collector.cacheMetrics.l1HitRate = alpha*1.0 + (1-alpha)*m.collector.cacheMetrics.l1HitRate
	} else {
		m.collector.cacheMetrics.l1HitRate = alpha*0.0 + (1-alpha)*m.collector.cacheMetrics.l1HitRate
	}

	if l2Hit {
		m.collector.cacheMetrics.l2HitRate = alpha*1.0 + (1-alpha)*m.collector.cacheMetrics.l2HitRate
	} else {
		m.collector.cacheMetrics.l2HitRate = alpha*0.0 + (1-alpha)*m.collector.cacheMetrics.l2HitRate
	}

	// Record latencies
	if l1Latency > 0 {
		m.collector.cacheMetrics.l1Latencies = append(m.collector.cacheMetrics.l1Latencies, l1Latency)
		if len(m.collector.cacheMetrics.l1Latencies) > 100 {
			m.collector.cacheMetrics.l1Latencies = m.collector.cacheMetrics.l1Latencies[1:]
		}
	}

	if l2Latency > 0 {
		m.collector.cacheMetrics.l2Latencies = append(m.collector.cacheMetrics.l2Latencies, l2Latency)
		if len(m.collector.cacheMetrics.l2Latencies) > 100 {
			m.collector.cacheMetrics.l2Latencies = m.collector.cacheMetrics.l2Latencies[1:]
		}
	}
}

// RecordCustomMetric records a custom metric
func (m *PerformanceMonitor) RecordCustomMetric(name string, value float64, metricType string) {
	m.collector.mu.Lock()
	defer m.collector.mu.Unlock()

	metric, exists := m.collector.customMetrics[name]
	if !exists {
		metric = &CustomMetric{
			Name:   name,
			Type:   metricType,
			Values: make([]float64, 0, 100),
		}
		m.collector.customMetrics[name] = metric
	}

	metric.Value = value
	metric.LastUpdated = time.Now()

	if metricType == "histogram" {
		metric.Values = append(metric.Values, value)
		if len(metric.Values) > 100 {
			metric.Values = metric.Values[1:]
		}
	}
}

// GetCurrentAnalysis returns the current performance analysis
func (m *PerformanceMonitor) GetCurrentAnalysis() *AnalysisResult {
	m.analyzer.mu.RLock()
	defer m.analyzer.mu.RUnlock()

	if m.analyzer.currentAnalysis == nil {
		// Perform analysis now if not available
		return m.performAnalysis()
	}

	return m.analyzer.currentAnalysis
}

// performAnalysis analyzes current performance metrics
func (m *PerformanceMonitor) performAnalysis() *AnalysisResult {
	analysis := &AnalysisResult{
		Timestamp: time.Now(),
	}

	// Analyze response times
	m.collector.requestMetrics.mu.RLock()
	responseTimes := make([]time.Duration, len(m.collector.requestMetrics.responseTimes))
	copy(responseTimes, m.collector.requestMetrics.responseTimes)
	requestCount := m.collector.requestMetrics.requestCount
	errorCount := m.collector.requestMetrics.errorCount
	bytesTransferred := m.collector.requestMetrics.bytesTransferred
	elapsedTime := time.Since(m.collector.requestMetrics.startTime)
	m.collector.requestMetrics.mu.RUnlock()

	if len(responseTimes) > 0 {
		// Calculate percentiles
		sort.Slice(responseTimes, func(i, j int) bool {
			return responseTimes[i] < responseTimes[j]
		})

		analysis.P50ResponseTime = responseTimes[len(responseTimes)*50/100]
		analysis.P95ResponseTime = responseTimes[len(responseTimes)*95/100]
		analysis.P99ResponseTime = responseTimes[len(responseTimes)*99/100]
		analysis.MaxResponseTime = responseTimes[len(responseTimes)-1]

		// Calculate average
		var sum time.Duration
		for _, rt := range responseTimes {
			sum += rt
		}
		analysis.AvgResponseTime = sum / time.Duration(len(responseTimes))
	}

	// Calculate throughput
	if elapsedTime > 0 {
		analysis.RequestsPerSecond = float64(requestCount) / elapsedTime.Seconds()
		analysis.BytesPerSecond = float64(bytesTransferred) / elapsedTime.Seconds()
	}

	// Calculate error rate
	if requestCount > 0 {
		analysis.ErrorRate = float64(errorCount) / float64(requestCount)
	}

	// Cache analysis
	m.collector.cacheMetrics.mu.RLock()
	analysis.CacheEfficiency = (m.collector.cacheMetrics.l1HitRate + m.collector.cacheMetrics.l2HitRate) / 2
	m.collector.cacheMetrics.mu.RUnlock()

	// Generate recommendations
	analysis.Recommendations = m.generateRecommendations(analysis)

	// Calculate performance score (0-100)
	analysis.PerformanceScore = m.calculatePerformanceScore(analysis)

	return analysis
}

// generateRecommendations generates performance recommendations
func (m *PerformanceMonitor) generateRecommendations(analysis *AnalysisResult) []Recommendation {
	recommendations := []Recommendation{}

	// Check response times
	if analysis.P95ResponseTime > 1*time.Second {
		recommendations = append(recommendations, Recommendation{
			Type:        "response_time",
			Priority:    "high",
			Description: "P95 response time exceeds 1 second",
			Impact:      "Users experiencing slow responses",
			Action:      "Investigate slow endpoints, consider caching or optimization",
		})
	}

	// Check error rate
	if analysis.ErrorRate > 0.05 {
		recommendations = append(recommendations, Recommendation{
			Type:        "error_rate",
			Priority:    "high",
			Description: fmt.Sprintf("Error rate is %.2f%%", analysis.ErrorRate*100),
			Impact:      "Service reliability affected",
			Action:      "Review error logs and implement error handling improvements",
		})
	}

	// Check cache efficiency
	if analysis.CacheEfficiency < 0.7 {
		recommendations = append(recommendations, Recommendation{
			Type:        "cache_efficiency",
			Priority:    "medium",
			Description: fmt.Sprintf("Cache efficiency is %.2f%%", analysis.CacheEfficiency*100),
			Impact:      "Increased backend load",
			Action:      "Review cache configuration and warming strategies",
		})
	}

	return recommendations
}

// calculatePerformanceScore calculates an overall performance score
func (m *PerformanceMonitor) calculatePerformanceScore(analysis *AnalysisResult) float64 {
	score := 100.0

	// Deduct for slow response times
	if analysis.P95ResponseTime > 500*time.Millisecond {
		score -= math.Min(20, float64(analysis.P95ResponseTime.Milliseconds())/100)
	}

	// Deduct for errors
	score -= analysis.ErrorRate * 100

	// Deduct for poor cache performance
	score -= (1 - analysis.CacheEfficiency) * 20

	// Ensure score is between 0 and 100
	return math.Max(0, math.Min(100, score))
}

// setDefaultAlertThresholds sets default alert thresholds
func (m *PerformanceMonitor) setDefaultAlertThresholds() {
	m.alertManager.thresholds["response_time_p95"] = AlertThreshold{
		MetricName:     "response_time_p95",
		ThresholdValue: 2000, // 2 seconds
		Comparison:     "greater",
		Duration:       1 * time.Minute,
		Severity:       "warning",
	}

	m.alertManager.thresholds["error_rate"] = AlertThreshold{
		MetricName:     "error_rate",
		ThresholdValue: 0.1, // 10%
		Comparison:     "greater",
		Duration:       5 * time.Minute,
		Severity:       "critical",
	}

	m.alertManager.thresholds["cache_hit_rate"] = AlertThreshold{
		MetricName:     "cache_hit_rate",
		ThresholdValue: 0.5, // 50%
		Comparison:     "less",
		Duration:       10 * time.Minute,
		Severity:       "warning",
	}
}

// collectionWorker periodically collects metrics
func (m *PerformanceMonitor) collectionWorker() {
	defer m.wg.Done()

	ticker := time.NewTicker(m.config.CollectionInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			m.collectMetrics()
		case <-m.shutdown:
			return
		}
	}
}

// collectMetrics collects current metrics
func (m *PerformanceMonitor) collectMetrics() {
	// Collect system metrics
	// This would be implemented to gather actual system metrics
	m.collector.systemMetrics.mu.Lock()
	m.collector.systemMetrics.lastUpdated = time.Now()
	m.collector.systemMetrics.mu.Unlock()
}

// analysisWorker periodically analyzes performance
func (m *PerformanceMonitor) analysisWorker() {
	defer m.wg.Done()

	ticker := time.NewTicker(m.analyzer.analysisInterval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			analysis := m.performAnalysis()

			m.analyzer.mu.Lock()
			m.analyzer.currentAnalysis = analysis
			m.analyzer.mu.Unlock()

			// Check for alerts
			if m.config.EnableAlerting {
				m.checkAlerts(analysis)
			}

			// Store in history
			m.storeInHistory(analysis)

		case <-m.shutdown:
			return
		}
	}
}

// checkAlerts checks for alert conditions
func (m *PerformanceMonitor) checkAlerts(analysis *AnalysisResult) {
	// Check response time alert
	if threshold, exists := m.alertManager.thresholds["response_time_p95"]; exists {
		if float64(analysis.P95ResponseTime.Milliseconds()) > threshold.ThresholdValue {
			m.triggerAlert("response_time_p95", float64(analysis.P95ResponseTime.Milliseconds()), threshold)
		}
	}

	// Check error rate alert
	if threshold, exists := m.alertManager.thresholds["error_rate"]; exists {
		if analysis.ErrorRate > threshold.ThresholdValue {
			m.triggerAlert("error_rate", analysis.ErrorRate, threshold)
		}
	}
}

// triggerAlert triggers an alert
func (m *PerformanceMonitor) triggerAlert(metricName string, currentValue float64, threshold AlertThreshold) {
	alert := Alert{
		ID:           fmt.Sprintf("%s-%d", metricName, time.Now().UnixNano()),
		MetricName:   metricName,
		CurrentValue: currentValue,
		Threshold:    threshold.ThresholdValue,
		Severity:     threshold.Severity,
		TriggeredAt:  time.Now(),
		Message:      fmt.Sprintf("%s threshold exceeded: %.2f > %.2f", metricName, currentValue, threshold.ThresholdValue),
	}

	m.alertManager.mu.Lock()
	m.alertManager.activeAlerts[metricName] = &alert
	m.alertManager.alertHistory = append(m.alertManager.alertHistory, alert)
	m.alertManager.mu.Unlock()

	// Call alert callbacks
	for _, callback := range m.alertManager.alertCallbacks {
		go callback(alert)
	}

	m.logger.Warn("Performance alert triggered", map[string]interface{}{
		"metric":    metricName,
		"value":     currentValue,
		"threshold": threshold.ThresholdValue,
		"severity":  threshold.Severity,
	})
}

// storeInHistory stores analysis results in history
func (m *PerformanceMonitor) storeInHistory(analysis *AnalysisResult) {
	m.history.mu.Lock()
	defer m.history.mu.Unlock()

	// Store key metrics in time series
	m.addToTimeSeries("response_time_p95", float64(analysis.P95ResponseTime.Milliseconds()))
	m.addToTimeSeries("requests_per_second", analysis.RequestsPerSecond)
	m.addToTimeSeries("error_rate", analysis.ErrorRate)
	m.addToTimeSeries("cache_efficiency", analysis.CacheEfficiency)
	m.addToTimeSeries("performance_score", analysis.PerformanceScore)
}

// addToTimeSeries adds a data point to a time series
func (m *PerformanceMonitor) addToTimeSeries(name string, value float64) {
	series, exists := m.history.timeSeries[name]
	if !exists {
		series = &TimeSeries{
			Name:       name,
			DataPoints: make([]DataPoint, 0, 1000),
			MaxPoints:  1000,
		}
		m.history.timeSeries[name] = series
	}

	series.DataPoints = append(series.DataPoints, DataPoint{
		Timestamp: time.Now(),
		Value:     value,
	})

	// Trim old data points
	if len(series.DataPoints) > series.MaxPoints {
		series.DataPoints = series.DataPoints[len(series.DataPoints)-series.MaxPoints:]
	}
}

// GetMetrics returns comprehensive performance metrics
func (m *PerformanceMonitor) GetMetrics() map[string]interface{} {
	analysis := m.GetCurrentAnalysis()

	metrics := map[string]interface{}{
		"current_analysis": map[string]interface{}{
			"timestamp":           analysis.Timestamp,
			"avg_response_time":   analysis.AvgResponseTime.String(),
			"p50_response_time":   analysis.P50ResponseTime.String(),
			"p95_response_time":   analysis.P95ResponseTime.String(),
			"p99_response_time":   analysis.P99ResponseTime.String(),
			"requests_per_second": analysis.RequestsPerSecond,
			"bytes_per_second":    analysis.BytesPerSecond,
			"error_rate":          analysis.ErrorRate,
			"cache_efficiency":    analysis.CacheEfficiency,
			"performance_score":   analysis.PerformanceScore,
			"recommendations":     analysis.Recommendations,
		},
	}

	// Add active alerts
	m.alertManager.mu.RLock()
	activeAlerts := make([]map[string]interface{}, 0)
	for _, alert := range m.alertManager.activeAlerts {
		activeAlerts = append(activeAlerts, map[string]interface{}{
			"metric":    alert.MetricName,
			"value":     alert.CurrentValue,
			"threshold": alert.Threshold,
			"severity":  alert.Severity,
			"triggered": alert.TriggeredAt,
		})
	}
	m.alertManager.mu.RUnlock()

	metrics["active_alerts"] = activeAlerts

	return metrics
}

// RegisterAlertCallback registers a callback for alerts
func (m *PerformanceMonitor) RegisterAlertCallback(callback AlertCallback) {
	if m.alertManager != nil {
		m.alertManager.alertCallbacks = append(m.alertManager.alertCallbacks, callback)
	}
}

// Close shuts down the performance monitor
func (m *PerformanceMonitor) Close() error {
	close(m.shutdown)
	m.wg.Wait()
	return nil
}
